# ğŸ› ï¸ DataForge â€“ Automatisierte Datenpipeline mit Terraform & AWS ğŸš€

**DataForge** ist ein Infrastructure-as-Code-Projekt, das eine skalierbare, serverlose Datenpipeline auf AWS erstellt â€“ vollstÃ¤ndig automatisiert mit Terraform und Python.

## ğŸ“Œ Ziel
Daten automatisiert erfassen, speichern, transformieren â€“ und das cloudbasiert, versionierbar und erweiterbar.

---

## âš™ï¸ ArchitekturÃ¼bersicht

```text
+-------------+         +----------------+         +----------------+
| DynamoDB    |  --->   | AWS Lambda     |  --->   | S3-Bucket       |
| (Rohdaten)  |         | (Export)       |         | (Staging Layer) |
+-------------+         +----------------+         +----------------+
                                                         â†“
                                                  +------------------+
                                                  | AWS RDS (MySQL)  |
                                                  | (Persistenz)     |
                                                  +------------------+
                                                         â†“
                                                  +------------------+
                                                  | dbt-Modelle:     |
                                                  |  - fact_appointments |
                                                  |  - dim_patient        |
                                                  |  - dim_dentist        |
                                                  |  - dim_treatment      |
                                                  |  - appointment_financials |
                                                  |  - dim_date           |
                                                  +------------------+
                                                         â†“
                                                 [Power BI / QS]
```

---

## ğŸ”§ Technologien

- **Terraform** â€“ Infrastruktur als Code
- **AWS DynamoDB** â€“ Speicherung strukturierter & semi-strukturierter Daten
- **AWS Lambda (Python)** â€“ Datenexport & Integration
- **S3** â€“ Staging Layer fÃ¼r strukturierte Daten
- **AWS RDS (MySQL)** â€“ relationale Persistenz
- **dbt (Data Build Tool)** â€“ Transformation & semantische Anreicherung
- **GitHub** â€“ CI/CD & Versionierung
- **Power BI / QuickSight** â€“ Visualisierung

---

## ğŸ—‚ï¸ Projektstruktur

```
DataForge/
â”œâ”€â”€ lambda/                 # Lambda-Code (Python)
â”‚   â”œâ”€â”€ dynamo_to_s3.py
â”‚   â””â”€â”€ csv_to_rds.py
â”œâ”€â”€ scripts/                # Jobs & Hilfsskripte
â”‚   â”œâ”€â”€ export_to_csv.py
â”‚   â””â”€â”€ insert_data.py
â”œâ”€â”€ data/                   # JSON/CSV-Testdaten
â”‚   â””â”€â”€ csv/
â”‚       â”œâ”€â”€ fact_appointments.csv
â”‚       â””â”€â”€ structured_export.csv
â”œâ”€â”€ terraform/              # Infrastruktur mit Terraform
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ rds.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ dynamodb.tf
â”‚   â”œâ”€â”€ network.tf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                 # dbt-Modelle fÃ¼r DWH
â”‚   â”œâ”€â”€ appointment_financials.sql
â”‚   â”œâ”€â”€ dim_patient.sql
â”‚   â”œâ”€â”€ dim_dentist.sql
â”‚   â”œâ”€â”€ dim_treatment.sql
â””â”€â”€ README.md
```

---

## ğŸš€ Erste Schritte

```bash
terraform init
terraform apply
```

ğŸ“Œ Testdaten einfÃ¼gen:
```bash
python insert_data.py --records 10 --table all
```

---

## ğŸ“… Fortschritt & Kommende Features

- [x] Infrastruktur mit Terraform
- [x] Datenexport mit AWS Lambda
- [x] Zeitbasierter Trigger via EventBridge
- [x] Datenverarbeitung mit AWS Glue (frÃ¼here Implementierung â€“ entfernt)
- [x] SQL-Persistenz mit AWS RDS (MySQL)
- [x] CSV zu Parquet (ehemals Glue Job) â€“ wird neu gedacht
- [x] Datenintegration Ã¼ber Lambda in RDS (funktioniert, wird weiter verbessert)
- [x] Transformationen & Datenaufwertung mit dbt
- [x] Aufbau eines Mini-Data Warehouses (Fact & Dimensions)
- [x] Erstellung eines KPI-Views (appointment_financials) mit dbt
- [x] Visualisierung mit Power BI oder QuickSight
- [ ] CI/CD Pipeline mit GitHub Actions
- [ ] Visuelle Architektur-Doku (draw.io)
- [ ] Verbindung von QuickSight zur RDS Ã¼ber VPC-Verbindung finalisieren

### ğŸ§­ Geplante Erweiterungen (Phase 2 â€“ reloaded)

- Datenintegration via Lambda â†’ RDS (statt Glue)
- Visualisierung mit Power BI oder QuickSight
- GitHub Actions fÃ¼r CI/CD Checks & Deployment

---

## ğŸ“Š Datenquellen & Formate

- **Structured:** Sensor-Messdaten (ID, Timestamp, Value, Status) â†’ Parquet
- **JSON Events:** Benutzeraktionen (Login, Upload etc.) â†’ Parquet
- **Key-Value Configs:** Versionen & Parameter â†’ Parquet
- CSV-Dateien: Faktendaten zu Terminen (fact_appointments.csv) â†’ Parquet â†’ MySQL

UrsprÃ¼nglich wurden alle Daten via AWS Glue katalogisiert â€“ mittlerweile setzen wir auf einen schlanken Lambdaâ†’RDS Flow mit optionaler dbt-Transformation.

ZusÃ¤tzlich wurde eine Data Warehouse-Schicht auf Basis der Faktentabelle `fact_appointments` modelliert, inklusive der Dimensionstabellen `dim_patient`, `dim_dentist`, `dim_treatment` sowie einer analytischen View `appointment_financials`.
Die Transformationen erfolgen Ã¼ber dbt und kÃ¶nnen modular erweitert werden.

--- 

## ğŸ§  Autor
**Vadim Ott** â€“ Data Engineer in Progress ğŸ‘¨â€ğŸ’»  
ğŸ“ MÃ¼nchen Â· GitHub: [@voml77](https://github.com/voml77)

---

## ğŸ“¸ Screenshots / Diagramme (optional einfÃ¼gen)

> âš ï¸ Platzhalter â€“ hier kÃ¶nnen spÃ¤ter S3-Dateien, Terraform-Ausgaben, oder ein draw.io-Diagramm ergÃ¤nzt werden.
> ğŸ”§ Derzeit in Arbeit: draw.io-Diagramm zur End-to-End-Pipeline.

---

## ğŸ’¸ Hinweis zu Kosten / Ressourcen

Die AWS RDS Instanz lÃ¤uft dauerhaft, sofern sie nicht gestoppt oder gelÃ¶scht wird. Um unnÃ¶tige Kosten zu vermeiden:

- Nutze mÃ¶glichst die Free Tier-GrÃ¶ÃŸe (`db.t3.micro`) â€“ bereits gewÃ¤hlt
- RDS verursacht Kosten **auch im Leerlauf** â€“ ggf. regelmÃ¤ÃŸig stoppen
- Speicherplatz (z.â€¯B. 20 GB) wird ebenfalls berechnet

ğŸ”§ Empfehlung: Instanz manuell stoppen, wenn nicht aktiv verwendet (z.â€¯B. Ã¼ber die AWS Console)  
ğŸ” Hinweis: Wenn du die RDS-Instanz spÃ¤ter erneut aktivierst, prÃ¼fe:
- Ist sie wieder als â€Ã¶ffentlich zugÃ¤nglichâ€œ markiert (Publicly Accessible = Yes)?
- Ist die passende Inbound-Regel in der Security Group gesetzt (MySQL-Port 3306 fÃ¼r deine aktuelle IP)?

ğŸ“Œ Beachte: Nach dem Neustart der RDS-Instanz muss hÃ¤ufig erneut:
- â€Publicly Accessibleâ€œ auf â€Yesâ€œ gesetzt werden
- Eine Inbound-Regel fÃ¼r deine aktuelle IP-Adresse freigegeben werden (Port 3306, TCP)